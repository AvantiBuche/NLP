{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e9c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen â¦ thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency â¦ my entire team. I have to thank everyone from the very onset of my career â¦ To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e3a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen â\\x80¦ thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency â\\x80¦ my entire team. I have to thank everyone from the very onset of my career â\\x80¦ To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f019b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4c532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609d1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "for i in range(len(datasets)):\n",
    "    words = nltk.word_tokenize(datasets[i])\n",
    "    newwords = [word for word in words if word not in stopwords.words('english')]\n",
    "    datasets[i] = ' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2023ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef800f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank much .',\n",
       " 'Thank Academy .',\n",
       " 'Thank room .',\n",
       " 'I congratulate incredible nominees year .',\n",
       " 'The Revenant product tireless efforts unbelievable cast crew .',\n",
       " 'First , brother endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , talent screen surpassed friendship screen â\\x80¦ thank creating transcendent cinematic experience .',\n",
       " 'Thank everybody Fox New Regency â\\x80¦ entire team .',\n",
       " 'I thank everyone onset career â\\x80¦ To parents ; none would possible without .',\n",
       " 'And friends , I love dearly ; know .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets\n",
    "#removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a8443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datasets)):\n",
    "    datasets[i] = datasets[i].lower()\n",
    "    datasets[i] = re.sub(r'\\W',' ',datasets[i])\n",
    "    datasets[i] = re.sub(r'\\s+',' ',datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9654f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank much ',\n",
       " 'thank academy ',\n",
       " 'thank room ',\n",
       " 'i congratulate incredible nominees year ',\n",
       " 'the revenant product tireless efforts unbelievable cast crew ',\n",
       " 'first brother endeavor mr tom hardy ',\n",
       " 'tom talent screen surpassed friendship screen â thank creating transcendent cinematic experience ',\n",
       " 'thank everybody fox new regency â entire team ',\n",
       " 'i thank everyone onset career â to parents none would possible without ',\n",
       " 'and friends i love dearly know ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets\n",
    "#cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d597632",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for data in datasets: # 'thank you to the academy '\n",
    "    words = nltk.word_tokenize(data)# [thank, you, to, the, academy]\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a7c3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 6,\n",
       " 'much': 1,\n",
       " 'academy': 1,\n",
       " 'room': 1,\n",
       " 'i': 3,\n",
       " 'congratulate': 1,\n",
       " 'incredible': 1,\n",
       " 'nominees': 1,\n",
       " 'year': 1,\n",
       " 'the': 1,\n",
       " 'revenant': 1,\n",
       " 'product': 1,\n",
       " 'tireless': 1,\n",
       " 'efforts': 1,\n",
       " 'unbelievable': 1,\n",
       " 'cast': 1,\n",
       " 'crew': 1,\n",
       " 'first': 1,\n",
       " 'brother': 1,\n",
       " 'endeavor': 1,\n",
       " 'mr': 1,\n",
       " 'tom': 2,\n",
       " 'hardy': 1,\n",
       " 'talent': 1,\n",
       " 'screen': 2,\n",
       " 'surpassed': 1,\n",
       " 'friendship': 1,\n",
       " 'â': 3,\n",
       " 'creating': 1,\n",
       " 'transcendent': 1,\n",
       " 'cinematic': 1,\n",
       " 'experience': 1,\n",
       " 'everybody': 1,\n",
       " 'fox': 1,\n",
       " 'new': 1,\n",
       " 'regency': 1,\n",
       " 'entire': 1,\n",
       " 'team': 1,\n",
       " 'everyone': 1,\n",
       " 'onset': 1,\n",
       " 'career': 1,\n",
       " 'to': 1,\n",
       " 'parents': 1,\n",
       " 'none': 1,\n",
       " 'would': 1,\n",
       " 'possible': 1,\n",
       " 'without': 1,\n",
       " 'and': 1,\n",
       " 'friends': 1,\n",
       " 'love': 1,\n",
       " 'dearly': 1,\n",
       " 'know': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cede726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(6,word2count,key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9c1707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank', 'i', 'â', 'tom', 'screen', 'much']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8ac400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "\n",
    "word_idfs = {}\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in datasets:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count +=1\n",
    "    word_idfs[word] = np.log((len(datasets)/doc_count)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf4d918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 0.9808292530117263,\n",
       " 'i': 1.4663370687934272,\n",
       " 'â': 1.4663370687934272,\n",
       " 'tom': 1.791759469228055,\n",
       " 'screen': 2.3978952727983707,\n",
       " 'much': 2.3978952727983707}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ef4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "675b996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = {}\n",
    "for word in freq_words:\n",
    "    doc_tf = []\n",
    "    for data in datasets:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w == word:\n",
    "                frequency += 1\n",
    "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f759800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': [0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08333333333333333,\n",
       "  0.125,\n",
       "  0.08333333333333333,\n",
       "  0.0],\n",
       " 'i': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08333333333333333,\n",
       "  0.16666666666666666],\n",
       " 'â': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08333333333333333,\n",
       "  0.125,\n",
       "  0.08333333333333333,\n",
       "  0.0],\n",
       " 'tom': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.16666666666666666,\n",
       "  0.08333333333333333,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'screen': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0],\n",
       " 'much': [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f5e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value*word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aaa80d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.49041462650586315,\n",
       "  0.49041462650586315,\n",
       "  0.49041462650586315,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.08173577108431052,\n",
       "  0.12260365662646579,\n",
       "  0.08173577108431052,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2932674137586854,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1221947557327856,\n",
       "  0.2443895114655712],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1221947557327856,\n",
       "  0.1832921335991784,\n",
       "  0.1221947557327856,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2986265782046758,\n",
       "  0.1493132891023379,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39964921213306176, 0.0, 0.0, 0.0],\n",
       " [1.1989476363991853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "869df8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49041463, 0.49041463, 0.49041463, 0.        , 0.        ,\n",
       "        0.        , 0.08173577, 0.12260366, 0.08173577, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.29326741, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.12219476, 0.24438951],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.12219476, 0.18329213, 0.12219476, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.29862658, 0.14931329, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39964921, 0.        , 0.        , 0.        ],\n",
       "       [1.19894764, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(tfidf_matrix)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d28bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "370bfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(10,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d04055a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49041463, 0.49041463, 0.49041463, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08173577, 0.12260366, 0.08173577, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.29326741, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12219476, 0.24438951, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.12219476, 0.18329213, 0.12219476,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.29862658],\n",
       "       [0.14931329, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.39964921,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.19894764, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce5ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
